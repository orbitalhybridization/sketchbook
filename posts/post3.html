<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Situation Modeling vs. Language Modeling</title>
  <link rel="stylesheet" href="../css/style.css">
</head>
<body>
  <header>
    <h1>Blog</h1>
    <div id="header"></div>
    <script>
       fetch('../assets/header.html')
          .then(response => response.text())
          .then(data => {
             document.getElementById('header').innerHTML = data;
          })
          .catch(error => console.error('Error loading header:', error));
    </script> 
  </header>

  <main>
    <article>
      <h2>Situation Modeling vs. Language Modeling</h2>
      <p><em>Published </em></p>
      <p><em>Edited </em></p>

      <h3>Thought and Language</h3>
      <p>
       
        Observed human brain activity appears to reflect the usage of a representational space for multimodal, 
        contextual information. By multimodal, I mean this information is realized in multiple
        sensory modalities. By contextual, I mean that it is constrained by a higher level of a temporal heterarchy.

        This functionality has been most prominently seen in work on the default mode network (DMN), a network of brain regions 
        distributed across the frontal, parietal, and medial temporal lobes. Though originally known for its super-threshold 
        activation during resting periods between tasks (hence the name), further investigation has revealed that regions of the DMN 
        are particularly co-active during higher-order cognitive processes such as prospection and retrospection (thinking about the 
        future and the past), mind-wandering, imagination, and manipulating situation schemas. [cite] Thus, in the roughly two decades
        since its discovery, contemporary accounts label the DMN as a sort of engine for "situation modeling" or "sense-making" in
        the brain. 
        
      </p>

      <p>

        What initially piqued my interest about this network is a section in Mahowald et al. (2024)'s Dissociating Language and Thought
        in Large Language Models. [cite] In the paper, the authors argue for a distinction between formal linguistic competence, a capacity
        that large language models (LLMs) have largely achieved, and functional linguistic competence, which unaugmented LLMs still struggle with (though
        this weakness grows less apparent as the technology rapidly improves). The example of functional linguistic competence is simple situational
        modeling. Essentially, many facets of language are based on these non-linguistic systems, including but not limited to situation modeling
        (such as theory of mind, for example). This is what intrigues me.

        I began to wonder, how can we impact the representational space of a typical transformer model in
        the same way that the brain's language network is impacted by others? Can we achieve the same level of natural language use
        if we blend subsystems to reach a common communicative goal?
        
        
        In its simplest form, there are two augmentations that
        need to be made to the typical architecture: (1) An additional module for situation modeling, and (2) a functional, representational
        blending between the two systems. This is just one way to conceive of such a design, and it might not be the most efficient.

        That's why this is a blog post and not a journal article.

      </p>

      <p>

      <h3>Basic Design Considerations</h3>

        - Whilst the language model processes input in the typical, predictive fashion, another engine
        runs underneath. This engine is multimodal, temporally integrative, and has dynamic memory: a
        reservoir computing model.
        - The computation of the situation model is the integration of polymorphic information to
        influence the language model.
        - Somehow, the representational spaces must be "mixed," though kept separate for different
        functions of the system. How do these systems -- the linguistic and the situational -- interact in the brain? 
        Taking inspiration from this design would help. The angular gyrus seems to be a strong candidate.

        This is currently just an idea, but the implementation should come soon. Keep an eye out on this
        Github page!
      </p>

      <h3>Final Thoughts</h3>

      <p>
        
        The story that this line of inquiry seeks to investigate is not new; it is about natural language, how humans do it, and 
        how we can model the process in machines. However, I haven't found much of this brain-inspired approach as I'd like to see
        in the field.
        
        As Dale & Kello (2017) put it in their thought-provoking paper, "How Do Humans Make Sense?": <i>"beyond words, beyond sentences,
        there's a complex co-creation taking place when two people talk."</i> This non-linguistic part of communication is the engine
        beneath all of our naturalistic interaction, and I believe that blending functionally tied subsystems in the way the brain does 
        is a worthy manner of approach.

      </p>

    </article>
  </main>

  <footer>
    <p>Â© 2024 Ian L. Jackson</p>
  </footer>
</body>
</html>
